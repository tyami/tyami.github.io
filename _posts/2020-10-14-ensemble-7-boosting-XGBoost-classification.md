---
title: "ë¶€ìŠ¤íŒ… ì•™ìƒë¸” (Boosting Ensemble) 3-2: XGBoost for Classification"
excerpt: "Boosting ëª¨ë¸ ì¤‘ í•˜ë‚˜ì¸ XGBoostì˜ Classification ì•Œê³ ë¦¬ì¦˜ì„ ì •ë¦¬í•´ë´…ì‹œë‹¤"

categories:
- Machine learning

tags:
- Machine learning
- Ensemble
- Algorithm
- Boosting

toc: true
toc_sticky: true
toc_label: "XGBoost for Classification"

use_math: true
---

ì´ì „ ê¸€ ë³´ê¸°: [ë¶€ìŠ¤íŒ… ì•™ìƒë¸” (Boosting Ensemble) 3-1: XGBoost for Regression]({{ site.url }}{{ site.baseurl }}/machine%20learning/ensemble-6-boosting-XGBoost-regression/)

> ì´ì „ í¬ìŠ¤íŒ…ì—ì„œëŠ” XGBoost for Regression ì•Œê³ ë¦¬ì¦˜ì„ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤.  
> ì´ë²ˆ í¬ìŠ¤íŒ…ì—ì„œëŠ” XGBoost for Classification ì•Œê³ ë¦¬ì¦˜ì„ ì •ë¦¬í•´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

## XGBoost for Classification

XGBoost for Classificationì€ [Gradient Boosting for Classification]({{ site.url }}{{ site.baseurl }}/machine%20learning/ensemble-5-boosting-gradient-boosting-classification/)ê³¼ [XGBoost for Regression]({{ site.url }}{{ site.baseurl }}/machine%20learning/ensemble-6-boosting-XGBoost-regression/)ì— ëŒ€í•œ ì´í•´ê°€ ê¸°ë³¸ìœ¼ë¡œ ê¹”ë ¤ìˆì–´ì•¼ í•©ë‹ˆë‹¤. ë‘ ë‚´ìš©ì„ ëª¨ë‘ ì´í•´í•˜ê³  ìˆë‹¤ë©´, ì´ë²ˆ í¬ìŠ¤íŒ…ì€ ì´í•´í•˜ê¸° ì‰½ìŠµë‹ˆë‹¤ ğŸ˜€

ì¶”ê°€ë¡œ [Oddsì™€ Log(Odds)]({{ site.url }}{{ site.baseurl }}/machine%20learning/machine-learning-1-odds-log-odds/)ì— ëŒ€í•œ ì´í•´ë„ í•„ìš”í•©ë‹ˆë‹¤.

> ìœ„ì˜ ì•Œê³ ë¦¬ì¦˜ê³¼ ê°œë…ì´ ë‚¯ì„¤ë‹¤ë©´ ì´ì „ í¬ìŠ¤íŒ…ë“¤ì„ ì°¸ê³ í•´ì£¼ì„¸ìš” !

XGBoost for Classificationì˜ í•™ìŠµ ê³¼ì •ì€ XGBoost for Regressionê³¼ ìœ ì‚¬í•©ë‹ˆë‹¤. ë‹¤ë§Œ Gradient Boosting for Classificationì²˜ëŸ¼ Probabilityì˜ residualì„ ì˜ˆì¸¡í•˜ëŠ” decision treeë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

![XGBoost for Classification procedure overview]({{ site.url }}{{ site.baseurl }}/assets/images/post/ML/2020-10-12-gradient-boosting-classification/2020-10-12-gradient-boosting-classification-procedure-overview.png)

1. Create a first leaf
2. Calculate pseudo-residuals of probability
3. Create a next tree to predict pseudo-residuals
   1. Similarity score of root node
   2. Separation based on Gain
   3. Complete decision tree with limitation of limitation of depth
   4. Prune the tree according to \\(\gamma\\)
   5. Calculate Output value (Representative value)
4. Calculate predicted probability
5. Repeat step 2-4

- (Test) Scale, add up the results of each tree, and convert to probability

XGBoost for Regressionê³¼ ì „ì²´ì ì¸ íë¦„ì´ ê±°ì˜ ë¹„ìŠ·í•˜ì§€ë§Œ, Step 4ê°€ ì¶”ê°€ëœ ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆì œ ë°ì´í„°ë¡œ ì „ ê³¼ì •ì„ ì²œì²œíˆ ì •ë¦¬í•´ë´…ì‹œë‹¤.

![XGBoost for Classification ì˜ˆì‹œ ë°ì´í„°]({{ site.url }}{{ site.baseurl }}/assets/images/post/ML/2020-10-14-XGBoost-Classification/2020-10-14-XGBoost-Classification-example-data.png)

ì„¤ëª…ì— ì‚¬ìš©ë  ì˜ˆì‹œ ë°ì´í„°ì…ë‹ˆë‹¤. Drug dosageì— ë”°ë¼ ì•½ì´ íš¨ê³¼ì ì¼ì§€ êµ¬ë¶„í•˜ëŠ” binary classificatioë¬¸ì œë¡œ, ì´ 4ê°œì˜ ìƒ˜í”Œì´ ìˆìŠµë‹ˆë‹¤.

### 1. Create a first leaf

![Step 1: a first leaf]({{ site.url }}{{ site.baseurl }}/assets/images/post/ML/2020-10-14-XGBoost-Classification/2020-10-14-XGBoost-Classification-step1-first-leaf.png)

XGBoost for Classificationì—ì„œë„ ì—¬ëŠ Gradient Boostingì²˜ëŸ¼ leafë¡œ ì‹œì‘í•©ë‹ˆë‹¤. leafì˜ ì˜ˆì¸¡ê°’ì€ drugê°€ íš¨ê³¼ì ì¼ì§€ ë§ì¶”ëŠ” **í™•ë¥ ê°’**ìœ¼ë¡œ, ë””í´íŠ¸ ê°’ì€ 0.5ì…ë‹ˆë‹¤.

### 2. Calculate pseudo-residuals of probability

![Step 2: pseudo-residual of probability]({{ site.url }}{{ site.baseurl }}/assets/images/post/ML/2020-10-14-XGBoost-Classification/2020-10-14-XGBoost-Classification-step2-pseudo-residual.png)

ì²« leaf ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ì´ ì •í•´ì¡Œìœ¼ë‹ˆ, ì‹¤ì œê°’ - ì˜ˆì¸¡ê°’, ì¦‰ pseudo-residualì„ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Gradient Boosting for Classificationì—ì„œì™€ ë™ì¼í•˜ê²Œ, **Probabilityì˜ residual**ì„ ê³„ì‚°í•©ë‹ˆë‹¤.

### 3. Create a next tree to predict pseudo-residuals

#### 3-1. Similarity score of root node

![Step 3-1: Similarity ì •ì˜ëŠ” ë¶„ëª¨ê°€ ë‹¤ë¦…ë‹ˆë‹¤]({{ site.url }}{{ site.baseurl }}/assets/images/post/ML/2020-10-14-XGBoost-Classification/2020-10-14-XGBoost-Classification-step3-1-similarity.png)

Similarity scoreê°€ ë‹¤ë¥´ê²Œ ì •ì˜ë©ë‹ˆë‹¤. ë³µì¡í•˜ê²Œ ìƒê²¼ì§€ë§Œ XGBoost for Regressionì˜ similarity scoreì—ì„œ ë¶„ëª¨ë¶€ë¶„ë§Œ ë°”ë€ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë°”ë€ ë¶€ë¶„ ë˜í•œ Gradient Boosting for Classificationì—ì„œ ë³¸ log(odds) -> í™•ë¥  ë³€í™˜ ìˆ˜ì‹ê³¼ ë¹„ìŠ·í•˜ê²Œ ìƒê²¼ìŠµë‹ˆë‹¤. 

\\[
Similarity\; score=\frac{(\sum Residual_i)^2}{\sum[Previous\; probability_i \times (1-Previous\; probability_i)]+\lambda}
\\]

#### 3-2. Separation based on Gain

![Step 3-2: Gain ì •ì˜ëŠ” ë™ì¼í•©ë‹ˆë‹¤]({{ site.url }}{{ site.baseurl }}/assets/images/post/ML/2020-10-14-XGBoost-Classification/2020-10-14-XGBoost-Classification-step3-2-gain.png)

Gainì˜ ì •ì˜ëŠ” ë™ì¼í•©ë‹ˆë‹¤. ìµœëŒ€ì˜ Gainì„ ê°–ëŠ” ë¶„ê¸° ì¡°ê±´ì„ ì°¾ì•„ ë¶„ê¸°ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.

#### 3-3. Complete decision tree with limitation of limitation of depth

![Step 3-3: Tree ì™„ì„±]({{ site.url }}{{ site.baseurl }}/assets/images/post/ML/2020-10-14-XGBoost-Classification/2020-10-14-XGBoost-Classification-step3-3-complete-tree.png)

ìµœì ì˜ Treeë¥¼ ì™„ì„±í–ˆìŠµë‹ˆë‹¤. ì˜ˆì‹œì—ì„œëŠ” limitation of depth ì¡°ê±´ì„ 2ë¡œ ì…‹íŒ…í–ˆê¸° ë•Œë¬¸ì— ë” ì´ìƒ ë¶„ê¸°í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

Depth ì™¸ì—ë„ ë‹¤ë¥¸ ì¡°ê±´ì„ ê±¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë°”ë¡œ `min_child_weight` ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” \\(Cover\\)ë¼ëŠ” ê²ƒì…ë‹ˆë‹¤.

#### Cover ì˜ ê°œë…

![Coverì˜ ê°œë…ê³¼ ê°€ì´ë“œ]({{ site.url }}{{ site.baseurl }}/assets/images/post/ML/2020-10-14-XGBoost-Classification/2020-10-14-XGBoost-Classification-step3-3-cover.png)

CoverëŠ” similarity scoreì—ì„œ ë¶„ëª¨ ì¤‘ \\(\lambda\\)ë¥¼ ì œì™¸í•œ termì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì´ ê°’ì´ \\(Cover\\) parameter ê°’ë³´ë‹¤ ì‘ì„ ê²½ìš°, í•´ë‹¹ leafëŠ” ê°€ì§€ì¹˜ê¸°ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.

XGBoost for Regressionì—ì„œëŠ” \\(Number\; of\; residuals\\)ì´ í¬ê¸° ë•Œë¬¸ì— \\(Cover\\) ê°’ì´ ì»¤ë„ ì–´ëŠ ì •ë„ ê´œì°®ì§€ë§Œ, Classificationì—ì„œëŠ” í™•ë¥ ì˜ ê³±ì´ë‹¤ë³´ë‹ˆ \\(Cover\\) ì— ë”°ë¼ ë§ì€ leafê°€ ê°€ì§€ì¹˜ê¸°ë‹¹í•˜ê²Œ ë©ë‹ˆë‹¤. ë”°ë¼ì„œ Classificationì—ì„œëŠ” \\(Cover=0\\) ìœ¼ë¡œ ì…‹íŒ…í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤ê³  í•©ë‹ˆë‹¤.

#### 3-4. Prune the tree according to \\(\gamma\\)

![Step 3-4: ê°€ì§€ì¹˜ê¸°]({{ site.url }}{{ site.baseurl }}/assets/images/post/ML/2020-10-14-XGBoost-Classification/2020-10-14-XGBoost-Classification-step3-4-pruning.png)

ê°€ì§€ì¹˜ê¸°ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤. ì˜ˆì‹œì—ì„œëŠ” ëª¨ë“  ë…¸ë“œê°€ ê°€ì§€ì¹˜ê¸°ê°€ ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.

#### 3-5. Calculate Output value (Representative value)

![Step 3-5: Output value ê³„ì‚°]({{ site.url }}{{ site.baseurl }}/assets/images/post/ML/2020-10-14-XGBoost-Classification/2020-10-14-XGBoost-Classification-step3-5-output-value.png)

ê° leafì— ëŒ€í•´ Output value (Representative value)ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. Similarity scoreì™€ ë¹„ìŠ·í•˜ê²Œ ìƒê¸´ ì´ ê°’ì€ log(odds)ê°’ìœ¼ë¡œ ìƒê°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ Gradient Boostingì˜ output value ë³€í™˜ ì‹ì—ì„œ \\(\lambda\\)ë§Œ ì¶”ê°€ëœ ì‹ì´ê¸°ë„ í•©ë‹ˆë‹¤.

\\[
Output\; value=\frac{\sum Residual_i}{\sum[Previous\; probability_i \times (1-Previous\; probability_i)]+\lambda}
\\]

### 4. Calculate predicted probability

![Step 4: Predicted probability 1]({{ site.url }}{{ site.baseurl }}/assets/images/post/ML/2020-10-14-XGBoost-Classification/2020-10-14-XGBoost-Classification-step4-predicted-probability-1.png)

first leafì˜ ì˜ˆì¸¡ê°’ë„ í™•ë¥ ê°’ì´ì—ˆìœ¼ë‹ˆ, log(odds)ë¡œ ë³€í™˜í•´ì¤ë‹ˆë‹¤. ê·¸ë¦¬ê³  ê° ëª¨ë¸ì˜ log(odds)ê°’ì— learning rate \\(\eta\\)ë¥¼ ê³±í•´ ëª¨ë‘ í•©ì³ì¤ë‹ˆë‹¤.

ë§ˆì§€ë§‰ìœ¼ë¡œ ë‹¤ì‹œ í™•ë¥ ë¡œ ë³€í™˜í•´ì£¼ë©´ ìƒˆë¡œìš´ predicted probabilityë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

![Step 4: Predicted probability 2]({{ site.url }}{{ site.baseurl }}/assets/images/post/ML/2020-10-14-XGBoost-Classification/2020-10-14-XGBoost-Classification-step4-predicted-probability-2.png)

ëª¨ë“  ìƒ˜í”Œì— ëŒ€í•´ ì´ ê³¼ì •ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

### 5. Repeat step 2-4

ê·¸ë¦¬ê³  Treeì˜ ìƒì„±ì„ ê³„ì†í•´ì¤ë‹ˆë‹¤.

maximum number of treeì— ë„ë‹¬í•˜ê±°ë‚˜, Residualì´ ì§€ì •í•œ threshold ì´í•˜ë¡œ ë–¨ì–´ì§€ë©´ ìƒì„±ì„ ë©ˆì¶¥ë‹ˆë‹¤.

### (Test) Scale, add up the results of each tree, and convert to probability

![Step 4: Predicted probability 2]({{ site.url }}{{ site.baseurl }}/assets/images/post/ML/2020-10-14-XGBoost-Classification/2020-10-14-XGBoost-Classification-test.png)

first leafì™€ ëª¨ë“  treeì˜ log(odds)ë¥¼ í•©ì¹œ í›„, probabilityë¡œ ë³€í™˜í•˜ì—¬ ìµœì¢… ì˜ˆì¸¡ê°’ì„ ì–»ì–´ëƒ…ë‹ˆë‹¤.

## Python code
Python ì—ì„œëŠ” `xgboost` libraryë¡œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.

- [XGBoost Docs](https://xgboost.readthedocs.io/en/latest/)

```python

```

---

> ë‹¤ìŒ í¬ìŠ¤íŒ…ì—ì„œëŠ” ë¹ ë¥¸ ì†ë„ì™€ ì¤€ìˆ˜í•œ ì„±ëŠ¥ì„ ìë‘í•˜ëŠ” Microsoftì˜ LightGBM ëª¨ë¸ì— ëŒ€í•´ ì •ë¦¬í•´ë³´ê² ìŠµë‹ˆë‹¤.

ë‹¤ìŒ ê¸€ ë³´ê¸°: [ë¶€ìŠ¤íŒ… ì•™ìƒë¸” (Boosting Ensemble) 4: LightGBM]({{ site.url }}{{ site.baseurl }}/machine%20learning/ensemble-8-boosting-LightGBM/)
